 # MiniGPT 

This project includes a small (1-10M parameters) GPT-style transfromer in PyTorch (without using nn.transformer) trained on Project Gutenberg text.  
Training is followed by a characterization of its induction heads through attention heatmaps and head ablations. 

---

## Contents

- `TransformerModel.py` — Includes definitions of self-attention with causal masking, the transformer block, and the GPT. 
- `MiniGPT.ipynb` — Loads Project Gutenberg data and transformer model file, defines tokenization, and trains the GPT.  
- `InductionHeads.ipynb` — Trains/evaluates a neural “next-step” policy on the generated proof dataset; can be used to test neural-guided proof search.   
- `utils.py` — Shared utilities and helper functions for plotting, computing cross-entropy with head ablations, and more. 
- `Data/` — Stored datasets (`.txt`) used by the notebooks (three Jane Austen novels).  
- `Plots/` — Plots of attention heatmaps and head ablations for different sized models. 

---

## Background

Classical symbolic theorem provers are reliable but can be slow because search branching explodes.
A common hybrid idea is: keep the symbolic verifier (so every step is checkable), but learn a neural policy that proposes the most promising next rule/action.

This repo implements that loop for *propositional logic*:
1) generate shortest proof traces with a symbolic prover (Z3 + custom rules),
2) train a neural network to imitate the next proof step,
3) use the neural net to guide proof search.

---

## Model architectures

This repo compares three next-step predictors:

- **MLP:** flattens (known formulas + goal) into a single vector and predicts the next rule/formula.
- **Deep Sets:** permutation-invariant over the set of known formulas, then conditions on the goal.
- **Transformer (encoder-only):** treats the known formulas as tokens and appends the goal as the final token.
  We use positional encoding **only on the final token (the goal)** so the model can distinguish “goal” from the unordered
  set of known formulas, while keeping the known-formula tokens effectively order-agnostic.


---

## Example (5-step proof)

Below is an example of a 5-step proof trace generated by the symbolic prover.
Each step lists the inference rule used and the newly derived statement added to `known`.

**Start:**

  Given ['A', 'D->E', 'E->C', 'B&D', 'C|D'], prove B&C

**Step 1** (∧-Elimination):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B']

**Step 2** (∧-Elimination):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D']

**Step 3** (→-Elimination / Modus Ponens):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E']

**Step 4** (→-Elimination / Modus Ponens):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E', 'C']

**Step 5** (∧-Introduction):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E', 'C', 'B&C'] ✅



## Results

The figure below shows **out-of-distribution (OOD) generalization** in proof depth: when models are trained only on shallow proofs (e.g. Depth ≤ 1, blue), accuracy drops sharply as we evaluate on deeper proofs (Depth 3–5), indicating a clear distribution shift. 

But as training data includes deeper proofs (Depth ≤ 2,3,4,5), performance recovers to near-perfect accuracy across depths.  **Deep Sets** appears slightly more robust than an MLP under this depth shift, consistent with the fact that it is permutation-invariant and therefore doesn’t waste capacity learning an arbitrary ordering of the “known” statements in the input. The encoder-only Transformer is marginally strongest overall, probably because self-attention can model inter-statement (“token–token”) interactions directly when deciding which next step to apply.


### Accuracy vs Depth
![Accuracy vs Depth](Plots/Accuracy_vs_Depth.png)

### Accuracy vs Depth (easier to see separated ...)
![Accuracy vs Depth](Plots/Accuracy_vs_Depth_Separate.png)

---

## Installation

```bash
git clone https://github.com/sferrante/Neuro-Symbolic-Reasoning.git
cd Neuro-Symbolic-Reasoning

pip install numpy
pip install matplotlib
pip install sklearn
pip install z3-solver
pip install torch --index-url https://download.pytorch.org/whl/cpu
