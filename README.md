 # MiniGPT 

A small (1-10M parameters) GPT-style language model in **PyTorch** from scratch.
The model is trained on **Project Gutenberg** text (Jane Austen novels), then analyzed with **mechanistic interpretability** tools—attention heatmaps and targeted head ablations—to identify and validate **induction heads**.

---

## Overview

- **Model:** decoder-only GPT-style Transformer (token+pos embeddings, causal multi-head self-attention, MLP blocks, residuals/LayerNorm) implemented from scratch in PyTorch (no `nn.Transformer`).
- **Training:** trained on public-domain Project Gutenberg text (Jane Austen novels).
- **Analysis:** mechanistic interpretability study of **induction heads** using attention heatmaps and targeted head ablations (measuring loss impact).

--- 

## Contents

- `TransformerModel.py` — Causal self-attention (masking), Transformer blocks, and the GPT model definition.
- `MiniGPT.ipynb` —  Data loading + tokenization, training loop, evaluation, and checkpoint saving.
- `InductionHeads.ipynb` — Induction-head investigation: attention heatmaps, head ablations, and quantitative loss/behavior checks.
- `utils.py` — Shared helpers (plotting, ablation utilities, cross-entropy evaluation, etc.).
- `Data/` — Training text (`.txt`) from Project Gutenberg (three Jane Austen novels).
- `Plots/` — Generated attention heatmaps + ablation visualizations for different model sizes.
- `miniGPT_2Mparams.pt` — Example saved checkpoint (model weights + config) for a 2M-parameter run.



---

## Example (5-step proof)

Below is an example of a 5-step proof trace generated by the symbolic prover.
Each step lists the inference rule used and the newly derived statement added to `known`.

**Start:**

  Given ['A', 'D->E', 'E->C', 'B&D', 'C|D'], prove B&C

**Step 1** (∧-Elimination):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B']

**Step 2** (∧-Elimination):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D']

**Step 3** (→-Elimination / Modus Ponens):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E']

**Step 4** (→-Elimination / Modus Ponens):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E', 'C']

**Step 5** (∧-Introduction):

  known : ['A', 'D->E', 'E->C', 'B&D', 'C|D', 'B', 'D', 'E', 'C', 'B&C'] ✅



## Results

The figure below shows **out-of-distribution (OOD) generalization** in proof depth: when models are trained only on shallow proofs (e.g. Depth ≤ 1, blue), accuracy drops sharply as we evaluate on deeper proofs (Depth 3–5), indicating a clear distribution shift. 

But as training data includes deeper proofs (Depth ≤ 2,3,4,5), performance recovers to near-perfect accuracy across depths.  **Deep Sets** appears slightly more robust than an MLP under this depth shift, consistent with the fact that it is permutation-invariant and therefore doesn’t waste capacity learning an arbitrary ordering of the “known” statements in the input. The encoder-only Transformer is marginally strongest overall, probably because self-attention can model inter-statement (“token–token”) interactions directly when deciding which next step to apply.


### Accuracy vs Depth
![Accuracy vs Depth](Plots/Accuracy_vs_Depth.png)

### Accuracy vs Depth (easier to see separated ...)
![Accuracy vs Depth](Plots/Accuracy_vs_Depth_Separate.png)

---

## Installation

```bash
git clone https://github.com/sferrante/Neuro-Symbolic-Reasoning.git
cd Neuro-Symbolic-Reasoning

pip install numpy
pip install matplotlib
pip install sklearn
pip install z3-solver
pip install torch --index-url https://download.pytorch.org/whl/cpu
